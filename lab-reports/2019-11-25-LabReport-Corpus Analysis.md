# Lab Report: Corpus Analysis

#### Lilly Wilcox

## Process Description

In this lab, we worked with rstudio to learn about different methods of corpus analysis. In the previous rstudio lab, we focused on tools like ngrams that helped us make assumptions about smaller data sets, like a particular text or an author's small corpus. In this lab, we broadened our scope, looking at 150 science fiction books in Project Gutenberg. We did a sentiment analysis of these books to assess which books were the angriest science fiction books, based on a sentiment analysis dictionary. After we looked at this, we moved on to topic modeling, which seeks to determine different "topics" by assesing what words occur frequently in the same text. Because this process only works with such a large corpus, we didn't get to explore topic analysis beyond just a conceptual discussion.

## Observations

Based on the previous rstudio lab that we did, my understanding of code as a type of literacy and the value of code in the social sciences had been established. This lab only expanded upon that perception. I was blown away by the relative speed with which an analysis of 150 books was presented. It's a but mindblowing how much data is at our fingertips and how easily we can sort and analyze that data. 

As a class, we looked at the sentiment analysis for the nine most angry novels in our science fiction corpus. Then, I ran a sentiment analysis based on the sentimnent "joy." One thing that I noticed was that the book, *The Monster Men* by EF Burroughs appeared on both the anger list and the joy list. I couldn't help but second guess my first observation based on this. How can a book be both so angry and so joyful? Is this data accurate? To answer my own questions, I think that it is possible for a book to be both angry and joyful -- it depends on the plot of the novel. Also, I don't think this renders the data obsolete, but it is important to remember that this analysis was done by a machine, which can't really read context. Words can be assigned sentiments, but the meaning of those words can vary. It is important to remember that this system has flaws. 

Topic modeling is an interesting tool to deal with masses of data. Again, its hard to wrap my head around a computer making seemingly such human associations and create relatively accurate categories. However, this is a tool that extends beyond human capability of data analysis. The amount of data used is massive and a human could not get through that much data. Additionally, this particular strategy seems like it would eliminate a lot of human error. A human couldn't find these frequency-based associations and would have to rely on personal bias to find associations. Topic modeling generally seems like a more reliable code-based analysis than sentiment analysis because it doesn't incorportate emotion and only relies on the computation of data. 

## Analysis

The reading "What Gets Counted Counts" focused a lot on binaries in code and how they are not as necessary as they seem. Sorting data into two categories has been understood as simplest, most effective way to make assumptions about a population. While we weren't looking specifically at binaries in this lab, I have similar thoughts about the efficacy of sentiment analysis. It's affordances are clear -- we can make assumptions about a massive data set, which is something a human couldn't do -- however, the limitations of sentiment analysis are less clear to me. 

As I expressed earlier, I know that context matters to the meaning of words. We talked in class about how sentiment analysis can't account for things like irony or sarcasm. However, an individual's gender identity does not vary much and when it does, terms like "genderfluid" can account for those variations. When an individual assigns their own gender to a data variable, the data analysis can't be limited by context. This limitation seen in sentiment analysis, where there are more than two categorical variables, would not apply to a data analysis of gender, which could have more than two categorical variables. 

Sentiment analysis seems to be relatively effective because it is not limited to a binary in the way that things like gender often are. While positive/negative sentiment analyses occur, this lab has proved to me that it's not difficult to conduct a categorization with more than two options, something that represents complexity. In our case, anger is more telling than negative and joy is more telling than positive. As a human, looking at data anysis by a computer, we can make more accurate assumptions about a dataset when there are a variety of variables. This would be the same for folks looking to do a data analysis related to gender. The reading suggests that binary gender representation in data is driven by corporations that want to target their advertising. However, wouldn't it beneficial to have a greater understanding of the nuances and complexities of gender in order to make more accurate assumptions? 

Based on this lab, to say that multiple categorical options to define gender in data is to complex is simplistic and incorrect. My understanding of categorical variables based on this lab lines up with the argument proposed in "What Gets Counted Counts." I appreciate the work that we did in class for how it contributed to my understanding of data and code in the context of the study of English and in the context of real-world problems and social change. 

